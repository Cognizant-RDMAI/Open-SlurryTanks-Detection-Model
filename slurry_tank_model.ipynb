{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3af50cb-8ed7-4098-92b4-a1e99e0e365d",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "The initial dataset was downloded following https://www.nature.com/articles/s41597-023-02780-1. The authors created a detailed dataset of over 130,000 above-ground storage tanks (ASTs) in the contiguous United States using high-resolution imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1a4fc-e4ad-4146-af33-ae772215fa57",
   "metadata": {},
   "source": [
    "\n",
    "## Extracting the Unique Classes from the Dataset\n",
    "\n",
    "\n",
    "To understand the dataset composition, the annotation files provided in PASCAL Visual Object Classes (VOC) 2007 format as Extensible Markup Language (XML) files had to be examined\n",
    "\n",
    "This code extracts all the unique classes present\n",
    "\n",
    "The output will be a list of distinct class labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4ce44-2725-4bbc-b563-542087328ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#To extract the unique classes from the XML file\n",
    "\n",
    "#importing required libraries\n",
    "import os\n",
    "import xml.etree.ElementTree as ET  #module for parsing and creating xml data\n",
    "\n",
    "def extract_classes_from_xml(xml_folder):\n",
    "    classes = set()  # Use a set to store unique class names\n",
    "    #iterating over XML files\n",
    "    for xml_file in os.listdir(xml_folder):\n",
    "        if xml_file.endswith(\".xml\"):\n",
    "            xml_path = os.path.join(xml_folder, xml_file)\n",
    "            if os.path.getsize(xml_path) == 0:  # Skip empty files\n",
    "                print(f\"Skipping empty file: {xml_file}\")\n",
    "                continue\n",
    "            #Parsing XML files\n",
    "            try:\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "                for obj in root.findall(\"object\"):\n",
    "                    #print(f\"{obj}\")\n",
    "                    class_name = obj.find(\"name\").text  #Extracts the text content of the <name> element within each <object>\n",
    "                    classes.add(class_name)  # Add class name to the set\n",
    "            except ET.ParseError:    #catches and handles any parsing error, skipping invalid files\n",
    "                print(f\"Skipping invalid file: {xml_file}\")\n",
    "    return sorted(list(classes))  # Return sorted list of unique classes\n",
    "\n",
    "xml_folder = \"datasets/xml_dataset_tank/xmls\"  # Path to XML files\n",
    "classes = extract_classes_from_xml(xml_folder)\n",
    "print(f\"Unique classes found: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053f324-a106-497d-98a9-e69e52211bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Converting Pascal VOC XML Annotations to YOLO Format\n",
    "\n",
    "Once the classes present in the dataset are identified, the annotation files in Pascal VOC(XML) format needs to be converted to YOLO format(TXT) to make it compatible with YOLO Object Detection model\n",
    "\n",
    "This code converts each XML annotation file into YOLO format which consists of text files with object lables and normalised bounding box coordinates\n",
    "\n",
    "### Understanding the formats:\n",
    "\n",
    "- Pascal VOC:\n",
    "Stores object annotations in an XML file, including class name, bounding box (xmin, ymin, xmax, ymax), image size and other metadata\n",
    "\n",
    "- YOLO (TXT) Format:\n",
    "Each image get a `.txt` file with the following format per line:\n",
    "\n",
    "```\n",
    "class_id, center_x, center_y, width, height\n",
    "```\n",
    "\n",
    "- `class_id`: Integer ID of the object class\n",
    "- `center_x, center_y`: Center of bounding box (normalised to image dimensions)\n",
    "- `width, height`: Bounding box size (normalised to image dimensions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6542138-c53c-491f-bf12-f52edb03beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To convert XML annotations in Pascal VOC format to YOLO format\n",
    "\n",
    "#importing required libraries\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#defining the function\n",
    "def voc_to_yolo(xml_folder, output_folder, classes):\n",
    "    os.makedirs(output_folder, exist_ok=True) #creates the output folder if it doesn't already exist\n",
    "    #iterating over the XML files\n",
    "    for xml_file in os.listdir(xml_folder):\n",
    "        if xml_file.endswith(\".xml\"):\n",
    "            xml_path = os.path.join(xml_folder, xml_file)\n",
    "            if os.path.getsize(xml_path) == 0:  # Skip empty files\n",
    "                print(f\"Skipping empty file: {xml_file}\")\n",
    "                continue\n",
    "            try:\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                # Get image size\n",
    "                size = root.find(\"size\")\n",
    "                width = int(size.find(\"width\").text)\n",
    "                height = int(size.find(\"height\").text)\n",
    "\n",
    "                # Create YOLO label file\n",
    "                yolo_file = os.path.join(output_folder, os.path.splitext(xml_file)[0] + \".txt\") #constructs the path to YOLO label file\n",
    "                with open(yolo_file, \"w\") as f:       #opens the YOLO file for writing\n",
    "                    #Processing each object\n",
    "                    for obj in root.findall(\"object\"):\n",
    "                        class_name = obj.find(\"name\").text\n",
    "                        if class_name not in classes:  #checks if the class name is in the provided list of classes\n",
    "                            print(f\"Skipping unknown class: {class_name}\")\n",
    "                            continue\n",
    "                        class_id = classes.index(class_name)  #gets the index of the class name in the list of classes\n",
    "\n",
    "                        #extarcting the bounding box coordinates\n",
    "                        bbox = obj.find(\"bndbox\")\n",
    "                        xmin = int(bbox.find(\"xmin\").text)\n",
    "                        ymin = int(bbox.find(\"ymin\").text)\n",
    "                        xmax = int(bbox.find(\"xmax\").text)\n",
    "                        ymax = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "                        # Normalize coordinates\n",
    "                        x_center = ((xmin + xmax) / 2) / width\n",
    "                        y_center = ((ymin + ymax) / 2) / height\n",
    "                        bbox_width = (xmax - xmin) / width\n",
    "                        bbox_height = (ymax - ymin) / height\n",
    "\n",
    "                        # Write to YOLO format\n",
    "                        f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "            except ET.ParseError:\n",
    "                print(f\"Skipping invalid file: {xml_file}\")\n",
    "\n",
    "    print(f\"Conversion completed! YOLO labels saved in: {output_folder}\")\n",
    "\n",
    "# Function call\n",
    "voc_to_yolo(\n",
    "    xml_folder=\"datasets/xml_dataset_tank\",  # Path to XML files\n",
    "    output_folder=\"datasets/xml_dataset_tank\",  # Path to save YOLO labels\n",
    "    classes=[\n",
    "        'closed_roof_tank',\n",
    "        'external_floating_roof_tank',\n",
    "        'narrow_closed_roof_tank',\n",
    "        'sedimentation_tank',\n",
    "        'spherical_tank',\n",
    "        'undefined_object',\n",
    "        'water_tower'\n",
    "    ]  # Unique classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00acf8-ac3c-47fe-878a-47e2dd5f1385",
   "metadata": {},
   "source": [
    "## Downloading Slurry Tank Imagery and Annotations\n",
    "\n",
    "After preparing the inital dataset, moved to collecting specific imagery for slurry tanks. The appraoch involved\n",
    "\n",
    "1. Identifying Locations: Determined existing slurry tank locations using Open Street Map Datasets for England, Denmark and Wales\n",
    "\n",
    "2. Downloading High-Resolution Imagery for the identified locations were used\n",
    "\n",
    "3. Annotating Slurry Tank: Using Label Studio, the slurry tanks were manually labeled and annotations were exported directly in YOLO format to align with training requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7775f-b70d-4e4b-b067-1357b1c52b3b",
   "metadata": {},
   "source": [
    "## Data Augmentation: Expanding the Slurry Tank Dataset\n",
    "\n",
    "After collecting and annotating the slurry tank images, it was observed that the dataset size was very low for effective training. To improve model performance and to prevent overfitting, data augmentation techniques were applied to artificially increase the number of training samples\n",
    "\n",
    "The code provided below was used to perform augmentations\n",
    "\n",
    "The *albumentations* library was used to perform the augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f9f16-4225-4ead-b0ef-a078f7b64d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create augmented images from slurry tank dataset\n",
    "\n",
    "import os\n",
    "import cv2   #OpenCV library for image processing\n",
    "import albumentations as A  #Library for data augmentation\n",
    "from albumentations.core.composition import BboxParams  #used to define parameters for bounding box augmentations\n",
    "from tqdm import tqdm #library for showing progress bars\n",
    "\n",
    "#Loading YOLO labels\n",
    "def load_yolo_labels(label_file):\n",
    "\n",
    "    #Load bounding boxes from a YOLO label file and returns it as a list of bounding boxes\n",
    "    bboxes = []\n",
    "    with open(label_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 0:\n",
    "                continue  # Skip empty lines\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            bboxes.append([x_center, y_center, width, height, class_id])\n",
    "    return bboxes\n",
    "\n",
    "#Saving Augmented Lables\n",
    "def save_augmented_label(label_file, augmented_bboxes):\n",
    "\n",
    "    #Save augmented bounding boxes in YOLO format.\n",
    "    with open(label_file, \"w\") as file:\n",
    "        for bbox in augmented_bboxes:\n",
    "            class_id, x_center, y_center, width, height = bbox\n",
    "            file.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "#Augmenting and Saving Images\n",
    "def augment_and_save_images(input_images_dir, input_labels_dir, output_images_dir, output_labels_dir, augmentations, num_augmentations=3):\n",
    "\n",
    "    #Perform data augmentation on images and save augmented images and labels.\n",
    "    os.makedirs(output_images_dir, exist_ok=True)\n",
    "    os.makedirs(output_labels_dir, exist_ok=True)\n",
    "\n",
    "    image_files = [f for f in os.listdir(input_images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        image_path = os.path.join(input_images_dir, image_file)\n",
    "        label_file = os.path.join(input_labels_dir, os.path.splitext(image_file)[0] + \".txt\")\n",
    "\n",
    "        if not os.path.exists(label_file):\n",
    "            print(f\"Skipping {image_file}: No corresponding label file.\")\n",
    "            continue\n",
    "\n",
    "        # Load image and labels\n",
    "        image = cv2.imread(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "        bboxes = load_yolo_labels(label_file)\n",
    "\n",
    "        # Convert YOLO labels for albumentations\n",
    "        albumentations_bboxes = [\n",
    "            [x_center, y_center, bbox_width, bbox_height, class_id]\n",
    "            for x_center, y_center, bbox_width, bbox_height, class_id in bboxes\n",
    "        ]\n",
    "\n",
    "        for i in range(num_augmentations):\n",
    "            augmented = augmentations(\n",
    "                image=image,\n",
    "                bboxes=albumentations_bboxes,\n",
    "                class_labels=[bbox[4] for bbox in bboxes]\n",
    "            )\n",
    "            aug_image = augmented[\"image\"]\n",
    "            aug_bboxes = augmented[\"bboxes\"]\n",
    "\n",
    "            # Save augmented image\n",
    "            aug_image_file = f\"{os.path.splitext(image_file)[0]}_aug{i}.jpg\"\n",
    "            aug_image_path = os.path.join(output_images_dir, aug_image_file)\n",
    "            cv2.imwrite(aug_image_path, aug_image)\n",
    "\n",
    "            # Save augmented labels\n",
    "            aug_label_file = f\"{os.path.splitext(image_file)[0]}_aug{i}.txt\"\n",
    "            aug_label_path = os.path.join(output_labels_dir, aug_label_file)\n",
    "\n",
    "            save_augmented_label(aug_label_path, [\n",
    "                [class_id, x_center, y_center, width, height] for x_center, y_center, width, height, class_id in aug_bboxes\n",
    "            ])\n",
    "\n",
    "    print(f\"Augmentation complete. Augmented data saved to {output_images_dir} and {output_labels_dir}.\")\n",
    "\n",
    "#Function Call\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    input_images_dir = \"datasets/slurry_tank/images\"  # Update with images directory\n",
    "    input_labels_dir = \"datasets/slurry_tank/labels\"  # Update with labels directory\n",
    "    output_images_dir = \"datasets/slurry_tank/augmented_data/images\"\n",
    "    output_labels_dir = \"datasets/slurry_tank/augmented_data/labels\"\n",
    "\n",
    "    # Define augmentations\n",
    "    augmentations = A.Compose(\n",
    "        [\n",
    "            # Geometric\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Affine(scale=(0.9, 1.1), translate_percent=(0.05, 0.05), rotate=(-15, 15), p=0.5),  # Small shifts and rotations\n",
    "\n",
    "            # Photometric\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.3),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "            A.MotionBlur(blur_limit=5, p=0.2),\n",
    "\n",
    "            # Compression Artifacts\n",
    "            A.ImageCompression(quality_lower=50, quality_upper=100, p=0.2),\n",
    "        ],\n",
    "        bbox_params=BboxParams(\n",
    "            format=\"yolo\",  # Specify YOLO format\n",
    "            label_fields=[\"class_labels\"],  # Ensure class labels are linked\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Run augmentation\n",
    "    augment_and_save_images(\n",
    "        input_images_dir,\n",
    "        input_labels_dir,\n",
    "        output_images_dir,\n",
    "        output_labels_dir,\n",
    "        augmentations,\n",
    "        num_augmentations=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd145a-08a5-4c02-a8f4-b045755b95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note while running image augmentations the label file created has class_id output as \"7.0\" instead of \"7\"\n",
    "# This code block is being used to correct the class_id\n",
    "import os\n",
    "  \n",
    "labels_dir = 'datasets/slurry_tank/augmented_data/labels'\n",
    "  \n",
    "for file_name in os.listdir(labels_dir):\n",
    "    if file_name.endswith(\".txt\"):  # Process only .txt files\n",
    "        file_path = os.path.join(labels_dir, file_name)\n",
    " \n",
    "        \n",
    "        with open(file_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    " \n",
    "        \n",
    "        updated_lines = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if parts:  # Ensure it's not an empty line\n",
    "                if parts[0] == \"7.0\":  # Check if class ID is 1.0\n",
    "                    parts[0] = \"7\"  # Change it to 1\n",
    "                updated_lines.append(\" \".join(parts))  # Rebuild the line\n",
    " \n",
    "        \n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(\"\\n\".join(updated_lines) + \"\\n\")\n",
    " \n",
    "print(\"Class IDs updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d096c-33b6-4e5e-9d06-0b08a3009b4c",
   "metadata": {},
   "source": [
    "## Creating Image Mosaics for Slurry Tanks\n",
    "\n",
    "After applying basic augmentations, image mosaics were introduced to further enhance the dataset complexity. Mosaic augmentation combines multiple images into a single image, creating more varied backgrounds and object placement.\n",
    "\n",
    "The below code is used is used to create mosaiced images. The main functions includes:\n",
    "- Selecting images randomly\n",
    "- Resizing and arranging it in a grid\n",
    "- Adjusting the Bounding Box Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641febe-cae9-498f-ba28-8219e399275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create mosaiced images from the augmented slurry tank images\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def load_image_and_labels(image_path, labels_path):\n",
    "    \"\"\"\n",
    "    Load an image and its corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image.\n",
    "        labels_path (str): Path to the label file.\n",
    "\n",
    "    Returns:\n",
    "        image (ndarray): Loaded image.\n",
    "        labels (list): List of bounding boxes and class IDs.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    labels = []\n",
    "    if os.path.exists(labels_path):\n",
    "        with open(labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                class_id = int(float(parts[0]))\n",
    "                x_center, y_center, width, height = map(float, parts[1:])\n",
    "                labels.append([class_id, x_center, y_center, width, height])\n",
    "    return image, labels\n",
    "\n",
    "def adjust_labels(labels, x_offset, y_offset, scale_x, scale_y):\n",
    "    \"\"\"\n",
    "    Adjust bounding boxes for mosaicing.\n",
    "\n",
    "    Args:\n",
    "        labels (list): Original bounding boxes.\n",
    "        x_offset (float): Horizontal offset.\n",
    "        y_offset (float): Vertical offset.\n",
    "        scale_x (float): Horizontal scale factor.\n",
    "        scale_y (float): Vertical scale factor.\n",
    "\n",
    "    Returns:\n",
    "        adjusted_labels (list): Adjusted bounding boxes.\n",
    "    \"\"\"\n",
    "    adjusted_labels = []\n",
    "    for label in labels:\n",
    "        class_id, x_center, y_center, width, height = label\n",
    "        x_center = (x_center * scale_x) + x_offset\n",
    "        y_center = (y_center * scale_y) + y_offset\n",
    "        width *= scale_x\n",
    "        height *= scale_y\n",
    "        adjusted_labels.append([class_id, x_center, y_center, width, height])\n",
    "    return adjusted_labels\n",
    "\n",
    "def create_mosaic(images_dir, labels_dir, output_images_dir, output_labels_dir, grid_size, num_samples):\n",
    "    \"\"\"\n",
    "    Create mosaic images from training data.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): Directory containing images.\n",
    "        labels_dir (str): Directory containing label files.\n",
    "        output_images_dir (str): Directory to save mosaiced images.\n",
    "        output_labels_dir (str): Directory to save mosaiced labels.\n",
    "        grid_size (int): Number of rows and columns in the grid.\n",
    "        num_samples (int): Number of mosaiced images to create.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_images_dir, exist_ok=True)\n",
    "    os.makedirs(output_labels_dir, exist_ok=True)\n",
    "\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        selected_files = random.sample(image_files, grid_size * grid_size)\n",
    "        images = []\n",
    "        all_labels = []\n",
    "\n",
    "        for j, file in enumerate(selected_files):\n",
    "            image_path = os.path.join(images_dir, file)\n",
    "            label_path = os.path.join(labels_dir, f\"{os.path.splitext(file)[0]}.txt\")\n",
    "            image, labels = load_image_and_labels(image_path, label_path)\n",
    "\n",
    "            h, w, _ = image.shape\n",
    "            scale_x, scale_y = 1 / grid_size, 1 / grid_size\n",
    "            x_offset = (j % grid_size) * scale_x\n",
    "            y_offset = (j // grid_size) * scale_y\n",
    "\n",
    "            image = cv2.resize(image, (int(w * scale_x), int(h * scale_y)))\n",
    "            adjusted_labels = adjust_labels(labels, x_offset, y_offset, scale_x, scale_y)\n",
    "\n",
    "            images.append(image)\n",
    "            all_labels.extend(adjusted_labels)\n",
    "\n",
    "        # Create mosaic\n",
    "        rows = [\n",
    "            np.hstack(images[r * grid_size:(r + 1) * grid_size])\n",
    "            for r in range(grid_size)\n",
    "        ]\n",
    "        mosaic = np.vstack(rows)\n",
    "\n",
    "        # Save mosaic image\n",
    "        mosaic_image_path = os.path.join(output_images_dir, f\"mosaic_{grid_size}x{grid_size}_{i}.jpg\")\n",
    "        cv2.imwrite(mosaic_image_path, mosaic)\n",
    "\n",
    "        # Save mosaic labels\n",
    "        mosaic_label_path = os.path.join(output_labels_dir, f\"mosaic_{grid_size}x{grid_size}_{i}.txt\")\n",
    "        with open(mosaic_label_path, \"w\") as f:\n",
    "            for label in all_labels:\n",
    "                f.write(\" \".join(map(str, label)) + \"\\n\")\n",
    "\n",
    "    print(f\"{grid_size}x{grid_size} Mosaiced images saved in {output_images_dir}\")\n",
    "    print(f\"{grid_size}x{grid_size} Mosaiced labels saved in {output_labels_dir}\")\n",
    "\n",
    "# User inputs\n",
    "images_dir = \"datasets/augmented_data/images\"  # Path to image directory\n",
    "labels_dir = \"datasets/augmented_data/labels\"  # Path to label directory\n",
    "output_images_dir = \"datasets/mosaiced_data/images\"  # Path to save mosaiced images\n",
    "output_labels_dir = \"datasets/mosaiced_data/labels\"  # Path to save mosaiced labels\n",
    "num_samples = 50  # Number of mosaiced images to create\n",
    "\n",
    "# Run mosaic creation for different grid sizes\n",
    "for grid_size in [2, 3, 4, 5, 6, 7,8]:\n",
    "    create_mosaic(\n",
    "        images_dir,\n",
    "        labels_dir,\n",
    "        output_images_dir,\n",
    "        output_labels_dir,\n",
    "        grid_size,\n",
    "        num_samples=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394881d-3976-4a46-a87e-fb279cef74e5",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "\n",
    "The next step is to split the dataset into train, validation and test sets. This ensures that the model is trained on one subset, validated on another and finally evaluated on an unseen test set.\n",
    "\n",
    "The below code, splits the dataset and move the image and corresponding label files to their respective folder.\n",
    "\n",
    "Here first split the above ground storage tank into train, val and test. After this copy the slurry tank dataset for England and Denmark into the train folder labels and images to respective folder. Run augmentations on the Wales dataset. Split the slurry tank data from Wales for Validation and Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f96979-486f-4ec9-9d1e-0b3f8dfb25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To split the data for training, testing and validating\n",
    "\n",
    "#importing libraries\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split #function from sklearn to split datasets into training and testing sets\n",
    "\n",
    "def split_dataset(dataset_dir, output_dir, train_ratio, val_ratio, test_ratio):\n",
    "    # Update paths to point to images and labels subdirectories\n",
    "    images_dir = os.path.join(dataset_dir, \"images\")\n",
    "    labels_dir = os.path.join(dataset_dir, \"labels\")\n",
    "\n",
    "    # Validate directories\n",
    "    if not os.path.exists(images_dir) or not os.path.exists(labels_dir):\n",
    "        print(f\"Error: 'images' or 'labels' subdirectory not found in {dataset_dir}\")\n",
    "        return\n",
    "\n",
    "    # List all image files\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if len(image_files) == 0:\n",
    "        print(\"No image files found in the 'images' folder. Please check your dataset.\")\n",
    "        return\n",
    "\n",
    "    # Split dataset\n",
    "    train_files, temp_files = train_test_split(image_files, test_size=(1 - train_ratio))\n",
    "    val_files, test_files = train_test_split(temp_files, test_size=test_ratio / (val_ratio + test_ratio))\n",
    "\n",
    "    # Define output subdirectories\n",
    "    splits = {'train': train_files, 'val': val_files, 'test': test_files}\n",
    "    for split, files in splits.items():\n",
    "        split_images_dir = os.path.join(output_dir, split, \"images\")\n",
    "        split_labels_dir = os.path.join(output_dir, split, \"labels\")\n",
    "        os.makedirs(split_images_dir, exist_ok=True)\n",
    "        os.makedirs(split_labels_dir, exist_ok=True)\n",
    "\n",
    "        # Move files to respective directories\n",
    "        for file in files:\n",
    "            # Copy image\n",
    "            shutil.copy(os.path.join(images_dir, file), split_images_dir)\n",
    "\n",
    "            # Copy corresponding label\n",
    "            label_file = os.path.splitext(file)[0] + \".txt\"\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            if os.path.exists(label_path):\n",
    "                shutil.copy(label_path, split_labels_dir)\n",
    "            else:\n",
    "                print(f\"Warning: Label file for {file} not found. Skipping.\")\n",
    "\n",
    "    print(f\"Dataset split completed. Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")\n",
    "    print(f\"Data saved to {output_dir}\")\n",
    "\n",
    "# Define paths and ratios\n",
    "dataset_dir = \"datasets/xml_dataset_tank\"  # Path to the dataset containing 'images' and 'labels' folders\n",
    "output_dir = \"datasets/processed_data\"\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Ensure ratios sum to 1\n",
    "assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.0!\"\n",
    "#The small tolerance (1e-6) accounts for minor floating-point arithmetic errors that might occur when adding the ratios\n",
    "\n",
    "\n",
    "# Call the function\n",
    "split_dataset(dataset_dir, output_dir, train_ratio, val_ratio, test_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fc881-b38e-4c14-a023-8bf168577bb8",
   "metadata": {},
   "source": [
    "## Dataset Configuration: Generating a YAML File for YOLO Training\n",
    "\n",
    "A YAML configuration file is required for training the YOLO model. It provides the essential information about the dataset including class names and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423652c2-f39a-4925-8a2e-0f503913a5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a data.yaml file for YOLO dataset configuration\n",
    "\n",
    "#importing libraries\n",
    "import os\n",
    "\n",
    "#Constructs YAML content with paths and number of classes\n",
    "#writes YAML content to data.yaml\n",
    "def create_data_yaml(output_dir, train_path, val_path, test_path, classes):\n",
    "    yaml_content = f\"\"\"\n",
    "# YOLO Dataset Configuration\n",
    "path: ./processed_data\n",
    "train: {train_path}\n",
    "val: {val_path}\n",
    "test: {test_path}\n",
    "# Number of classes\n",
    "nc: {len(classes)}\n",
    "names: {classes}\n",
    "    \"\"\"\n",
    "    yaml_path = os.path.join(output_dir, 'data.yaml')\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        f.write(yaml_content.strip())\n",
    "    print(f\"`data.yaml` file created at {yaml_path}\")\n",
    "\n",
    "# Function call\n",
    "if __name__ == \"__main__\":\n",
    "    create_data_yaml(\n",
    "        output_dir=\"datasets/processed_data\",  # Path to processed dataset\n",
    "        train_path=\"./train/images\",    # Path to training images\n",
    "        val_path=\"./val/images\",        # Path to validation images\n",
    "        test_path=\"./test/images\",      # Path to testing images\n",
    "        classes=[\n",
    "            'closed_roof_tank',\n",
    "            'external_floating_roof_tank',\n",
    "            'narrow_closed_roof_tank',\n",
    "            'sedimentation_tank',\n",
    "            'spherical_tank',\n",
    "            'undefined_object',\n",
    "            'water_tower',\n",
    "            'slurry_tank'\n",
    "        ]  # Unique class names\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e571aca-4b8f-49bb-865b-024f913b5d36",
   "metadata": {},
   "source": [
    "# MODEL TRAINING\n",
    "\n",
    "The YOLOv8l model was used in training, with 300 epochs. A relatively small batch size was chosen based on available GPU memory. The learning rate schedule and optimizer were selected for stable convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733ed07-ad71-401d-b31e-f554a72e9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the data\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8l.pt\")      #required YOLO model\n",
    "\n",
    "# Train the model with optimized hyperparameters\n",
    "model.train(\n",
    "    data=\"./processed_data/data.yaml\",  # Path to dataset YAML\n",
    "    epochs=300, #More epochs for better results\n",
    "    batch=8,  # Adjust based on GPU memory\n",
    "    imgsz=640,  # Higher resolution for better detections\n",
    "    lr0=0.001,  # Initial learning rate\n",
    "    lrf=0.0001,  # Final learning rate\n",
    "    optimizer=\"AdamW\",  # Use AdamW for stability\n",
    "    weight_decay=0.0005,  # Regularization\n",
    "    patience=20,  # Stops if no improvement after 20 epochs\n",
    "    hsv_h=0.015,  # Hue augmentation\n",
    "    hsv_s=0.7,  # Saturation augmentation\n",
    "    hsv_v=0.4,  # Brightness augmentation\n",
    "    translate=0.1,  # Image translation\n",
    "    scale=0.5,  # Image scaling\n",
    "    flipud=0.1,  # Vertical flip probability\n",
    "    fliplr=0.5,  # Horizontal flip probability\n",
    "    mosaic=1.0,  # Mosaic augmentation\n",
    "    mixup=0.1,  # Mixup augmentation\n",
    "    dropout=0.05,  # Add dropout for better generalization\n",
    "    multi_scale=True,  # Train with different image sizes\n",
    "    workers=16,  # More workers for faster data loading\n",
    "    device=0,  # Use GPU\n",
    ")\n",
    "\n",
    "print(\"Training complete! Check the 'runs/train' folder for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9f03b-041c-40bf-927f-c841d529c38c",
   "metadata": {},
   "source": [
    "# USING THE TRAINED MODEL FOR INFERENCE\n",
    "\n",
    "This script run object detection on a folder of JPEG or PNG images using a trained YOLOv8 model. It saves detection results as:\n",
    "\n",
    "- Annotated images with bounding boxes\n",
    "- YOLO-format text label files\n",
    "- Confidence scores\n",
    "The detections are saved in a specified output directory for easy review or further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502cd7b-49b2-43ab-937f-a923fda31f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for jpeg, png images\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "def detect_objects():\n",
    "    # Path to the trained weights\n",
    "    weights_path = \"runs/detect/train/weights/best.pt\"\n",
    "\n",
    "    # Path to the images or folder for detection\n",
    "    source_path = \"runs/detect/predictions/results9/0cb8e0d7-tile18_127992_86433.jpg\"\n",
    "\n",
    "    # Output directory for saving results\n",
    "    output_dir = \"runs/detect/predictions\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = YOLO(weights_path)\n",
    "\n",
    "    # Run detection\n",
    "    results = model.predict(\n",
    "        source=source_path,\n",
    "        save=True,         # Save predictions\n",
    "        save_txt=True,     # Save results as YOLO format labels\n",
    "        save_conf=True,    # Save confidence scores in results\n",
    "        conf=0.5,          # Confidence threshold for predictions\n",
    "        iou=0.5,           # IoU threshold for non-max suppression\n",
    "        project=output_dir, # Folder to save the results\n",
    "        name=\"results\",    # Subfolder name\n",
    "        line_width =1,     # Width of bounding box lines\n",
    "        classes =[7],      #Restricts detection to class ID 7\n",
    "        device =0          #GPU\n",
    "    )\n",
    "\n",
    "    print(f\"Detection complete. Results saved to {os.path.join(output_dir, 'results')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_objects()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70cb07-c151-47ff-b337-4b86941162d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
